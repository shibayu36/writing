---
Title: Coursera Machine Learning Week1を受けた & メモ
Category:
- tech
Date: 2017-03-29T08:26:40+09:00
URL: https://blog.shibayu36.org/entry/2017/03/29/082640
EditURL: https://blog.hatena.ne.jp/shiba_yu36/shibayu36.hatenablog.com/atom/entry/10328749687231862093
---

自分の中で裏側よりの難しいタスクをアサインされる機会を増やしたいと思い、機械学習の勉強を始めることにした。そこでとりあえず良いとよく言われているCoursera Machine Learningの講義を受けることにした。

https://www.coursera.org/learn/machine-learning

ひとまずWeek1を終わらせたので、受けたときにとったメモを置いておく。

* Introduction
機械学習とはどのようなところに使われるかと、教師あり学習学習・教師なし学習について少し触れるような内容だった。

** 教師あり学習
- ラベル付きデータを与えて学習するもの
- 教師あり学習の中に、回帰問題と分類問題がある
-- 回帰問題 -> 連続値出力の予測
-- 分類問題 -> 離散値出力の予測
- 例1: 家の値段を予測する -> 回帰問題
- 例2: 乳がんを良性か悪性か判定する -> 分類問題

** 教師なし学習
- 教師なし学習とは、ラベルを与えられていないデータ集合を、分類するもの(?)
-- Wikipediaによると、「データの背後に存在する本質的な構造を抽出するために用いられる」らしい
- 例1: マーケティングのセグメントを自動的に分類する
- 例2: 二つの音声(人とか環境音とか)を分離する -> カクテルパーティ問題

* Model and Cost Function
線形回帰モデルをどのように構築するかについて紹介する内容だった。一つずつ単純化して説明してから複雑なものを考えるという形式をとって説明してくれるので非常に分かりやすかった。

** 線形回帰モデル
- 線形回帰問題とは、訓練データで学習することで、予測するための関数hを得ることが目的となる
- ひとまず[tex:{h_\theta(x) = \theta_0 + \theta_1 * x}]というモデルを考える
- hが１次関数であるものを線形回帰モデル、もしくは単回帰モデルと呼ぶ

** Cost Function(目的関数)
- Cost Functionを利用することで、h(x)を算出できる
- Cost Functionは以下のように二乗誤差の平均と考える
-- [tex:{J(\theta_0, \theta_1) = \dfrac {1}{2m} \displaystyle \sum {i=1}^m \left ( \hat{y}{i}- y_{i} \right)^2 = \dfrac {1}{2m} \displaystyle \sum {i=1}^m \left (h\theta (x_{i}) - y_{i} \right)^2}]
- Cost Functionが最小になればデータセットに近い直線が得られ、関数hのための[tex:{\theta_0}]と[tex:{\theta_1}]が分かる
- 等高線のような図解が分かりやすかった

* Parameter Learning
Gradient Descent(最急降下法)について習う。
- 最急降下法は、関数の傾きを利用して最小に近づけていく方法
- αという学習率というパラメータを用いて最小に近づけていく
- 最急降下法では、初期値によって局所解というものにたどり着いてしまい、最適な解が得られないことがある
- 線形回帰の問題であれば、局所解は存在せず、ただ一つ解が存在することになる

最急降下法のアルゴリズム
[tex:{\begin{align*} \text{repeat until convergence: } \lbrace & \newline \theta_0 := & \theta_0 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m}(h_\theta(x_{i}) - y_{i}) \newline \theta_1 := & \theta_1 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m}\left((h_\theta(x_{i}) - y_{i}) x_{i}\right) \newline \rbrace& \end{align*}}]

* Linear Algebra Review
線形代数の基礎中の基礎として、行列演算について教えてもらえる。

- 関数適用を行列とベクトルの掛け算として見るのが面白い
- 複数の関数適用なら行列と行列の掛け算と見る

* まとめ
Week1を受けての感想は、非常に分かりやすく解説されているということ。説明がうますぎて、一切前提知識がなくても理解することが出来た。

数学的な知識も今の段階では分かっていなくとも理解することが出来た。どこかで数学の知識がないことで詰まってしまったら、またその機会に学習したい。

専門用語も多いので英語だとかなり理解に苦しむが、日本語の字幕も用意されていたので特に問題はなかった。たまに日本語の字幕がずれていて一切わからないところがあったので、そこは諦めて気合で英語の字幕を見ると良い。

とりあえず受けてみたものの、どのくらいモチベーションを保って続けられるかわからない。とりあえずTensorFlowとかに触ってみるというのを先にやっても良いのかもと思った。
