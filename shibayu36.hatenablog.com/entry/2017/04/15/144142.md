---
Title: Coursera Machine Learning Week2の学習
Category:
- tech
Date: 2017-04-15T14:41:42+09:00
URL: https://blog.shibayu36.org/entry/2017/04/15/144142
EditURL: https://blog.hatena.ne.jp/shiba_yu36/shibayu36.hatenablog.com/atom/entry/10328749687235078599
---

　[http://blog.shibayu36.org/entry/2017/03/29/082640:title=前回] に引き続き、Coursera Machine Learning Week 2を受講した。

　前回は線形回帰モデルとは何か、最小化すべきCost Functionは何か、最急降下法とは何かについて学ぶことができた。Week 2の講義を受けるとさらに次のことを理解することができる。

- 多変量(x1, x2, x3, ...など変数が多数あるもの)の線形回帰をどのように考えれば良いか
- 最急降下法のための、正規化や学習率の決め方について
- 最急降下法でなくて、正規方程式を利用した線形回帰モデルの計算方法
- Octaveのチュートリアル

　課題もやってみたけど、行列演算を利用することで難しい方程式を一気に計算できるようになるというのが面白いところだった。以下メモを置いておく。

* Multivariable Linear Regression
** Multiple Features
- 多変量の線形回帰
- x0=1と仮定すると、多変量の予測関数は次のように表せる
-- [tex:\begin{align*}h_\theta(x) =\begin{bmatrix}\theta_0 \hspace{2em} \theta_1 \hspace{2em} ... \hspace{2em} \theta_n\end{bmatrix}\begin{bmatrix}x_0 \newline x_1 \newline \vdots \newline x_n\end{bmatrix}= \theta^T x\end{align*}]

** Gradient Descent For Multiple Variables
[tex:\begin{align*} & \text{repeat until convergence:} \; \lbrace \newline \; & \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_0^{(i)}\newline \; & \theta_1 := \theta_1 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_1^{(i)} \newline \; & \theta_2 := \theta_2 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_2^{(i)} \newline & \cdots \newline \rbrace \end{align*}]

なので

[tex:\begin{align*}& \text{repeat until convergence:} \; \lbrace \newline \; & \theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \; & \text{for j := 0...n}\newline \rbrace\end{align*}]

[tex:x_0]を1と置くことにより、全てを一括で表せるようになり、可愛い感じになった。

** Gradient Descent in Practice Ⅰ - Feature Scaling
- より収束しやすくするために、それぞれの変数を正規化する
- -1 <= x(i) <= 1くらいなら良い
-- 経験的に -3 <= x(i) <= 3, -1/3 <= x(i) <= 1/3くらいなら許容
- [tex:x_i := \dfrac{x_i - \mu_i}{s_i}]
-- μiがxiの全ての値の平均、siがrange = (max - min)

** Gradient Descent in Practice Ⅱ - Learning Rate
- ずっとcost functionが下がり続けるLearning Rateを選ぶこと
- Learning Rate αが小さすぎると、収束するのが遅くなる
- Learning Rate αが大きすぎると、cost functionが減らなくなり、収束しなくなる可能性がある


** Features and Polynomial Regression
- 複数のfeatureを一つにまとめることもできる。例えばfeature1 = 縦、feature2 = 横だとしたら、featureをまとめてfeature3=面積(=縦*横)とすることもできる。
- [tex:x_2 = x_1^2], [tex:x_3 = x_1^3]と変数を定義することもできる。こうすると多項式の回帰のグラフとなる

* Computing Parameter Analytically
** Normal Equation
- Normal Equation = 正規方程式
- Normal Equationでのθ計算: [tex:\theta = (X^T X)^{-1}X^T y]

Gradient DescentとNormal Equationのメリット・デメリット
|* Gradient Descent         |* Normal Equation                                            |
| 学習率αの計算が必要      | 学習率の計算が不要                                          |
| たくさんのiterationが必要 | iterateが必要ない                                           |
| 計算量がO ([tex:kn^2])    | O ([tex:n^3])の計算量である, [tex:X^TX]の転置行列計算が必要 |
| nが多くても計算できる     | nが多いと遅くなる                                           |

- n=10000を超えてくると、Normal Equationでは計算が大変になる


* Octave/Matlab Tutorial
** Basic Operations
- 等しくないは ~=で表す
- 1:0.1:2は 1から増分0.1で2まで増やした行ベクトルを表している(?)
- histによって分布を見ることが出来る
- help (command)でhelpを見れる

** Moving Data Around
- loadとsaveでファイルの入出力

** Plotting Data
- plot系
- imagesc(A), colorbar, colormap gray;

** Vectorization
- forループをベクトルの掛け算に変換することで、簡単にかつ高速に演算できる
