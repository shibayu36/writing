---
Title: elasticsearchにwikipediaのデータをインデックスする
Category:
- tech
- elasticsearch
Date: 2015-06-22T18:34:14+09:00
URL: https://blog.shibayu36.org/entry/2015/06/22/183414
EditURL: https://blog.hatena.ne.jp/shiba_yu36/shibayu36.hatenablog.com/atom/entry/8454420450097011829
---

elasticsearchの機能を試すように何かしらデータを入れてみたかったので、wikipediaのデータをelasticsearchに適当にインデックスしてみた。

* wikipediaのデータセット
wikipediaのデータセットは https://ja.wikipedia.org/wiki/Wikipedia:%E3%83%87%E3%83%BC%E3%82%BF%E3%83%99%E3%83%BC%E3%82%B9%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89:title 辺りからダウンロードできる。いろんなデータがあるが、今回はhttp://dumps.wikimedia.org/jawiki/latest/ の中にあるpages-article1.xml.bz2辺りを利用する。

* wikipediaデータのインデックス
https://github.com/shibayu36/wikipedia-elasticsearch にサンプルを置いてある。

perlで適当に書いているので、最初にcarton installだけしておいてください。

** elasticsearchのセットアップ
elasticsearch自体のセットアップをするためにscript/setupを呼ぶとそのディレクトリのelasticsearch/以下にセットアップしてくれるようにしてみた。内部的にはelasticsearchのtarを取ってきて展開しているのと、pluginを入れてみているだけ。

>||
carton exec -- perl script/setup
||<


** elasticsearchの起動
elasticsearch以下に置いてあるので、以下のコマンドで起動。

>||
$ elasticsearch/bin/elasticsearch
||<

** wikipedia用のmappingsを作成
今回はそれぞれのページのidとタイトルと本文をインデックスするようにしてみる。

>|perl|
$es->indices->create(
    index => 'wikipedia',
    body  => {
        settings => {
            index => {
                analysis => {
                    tokenizer => {
                        text_ja_tokenizer => {
                            type => 'kuromoji_tokenizer',
                            mode => 'search',
                        },
                    },
                    analyzer => {
                        text_ja_analyzer => {
                            tokenizer => 'text_ja_tokenizer',
                            type => 'custom',
                            filter => [
                                'kuromoji_part_of_speech',
                                'icu_normalizer',
                            ],
                            char_filter => [
                                'html_strip'
                            ],
                        },
                    },
                },
            },
        },
        mappings => {
            page => {
                properties => {
                    id    => +{ type => 'long', index => 'not_analyzed' },
                    title => +{ type => 'string', index => 'analyzed', analyzer => 'text_ja_analyzer' },
                    text  => +{ type => 'string', index => 'analyzed', analyzer => 'text_ja_analyzer' },
                },
            },
        },
    },
);
||<

上はjsonのAPI叩く部分をperlのインターフェースでやっているだけ。elasticsearchを起動した上で、このスクリプトを叩く。

>||
$ carton exec -- perl script/create-elasticsearch-index.pl
||<


** wikipediaのデータをインポート
あとはダウンロードして展開したデータをインポートしてみる。

>||
carton exec -- perl script/import-wikipedia.pl jawiki-latest-pages-articles1.xml
||<

ファイル全部読み込んでツリー構造を構築し、全部メモリに持っているので、あんまり良くない。実際にはもうちょっと工夫とかしたほうが良さそう。

* まとめ
今回はwikipediaのデータをelasticsearchにインデックスしてみたメモを書いた。本当は https://github.com/takumakanari/embulk-parser-xml こういうのを使えるともっと良かったのだけど、少し複雑なxmlをうまく扱えなかったので、perlのスクリプトを書いてみた。
